{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision as LoRA - Colab Test (A100)\n",
    "\n",
    "Teste r√°pido do sistema em A100 antes do treino completo em H100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "!git clone https://github.com/seu-usuario/vision-as-lora.git\n",
    "%cd vision-as-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install -q torch torchvision transformers peft datasets accelerate wandb pillow tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from configs.config import Config, ModelConfig, VisionConfig, LoRAConfig, TrainingConfig, DataConfig, ExperimentConfig\n",
    "\n",
    "test_config = Config(\n",
    "    model=ModelConfig(\n",
    "        model_name=\"HuggingFaceTB/SmolLM2-135M\",\n",
    "        torch_dtype=\"bfloat16\",\n",
    "        use_flash_attention=True,\n",
    "    ),\n",
    "    vision=VisionConfig(\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "    ),\n",
    "    lora=LoRAConfig(\n",
    "        rank=64,\n",
    "        alpha=128,\n",
    "        vision_layers=8,\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        dataset_name=\"nielsr/coco-captions\",\n",
    "        train_split=\"train[:1%]\",\n",
    "        val_split=\"validation[:1%]\",\n",
    "        num_workers=2,\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        output_dir=\"/content/drive/MyDrive/vision-lora-test\",\n",
    "        run_name=\"colab-test-a100\",\n",
    "        batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_length=128,\n",
    "        target_samples=1000,\n",
    "        learning_rate=5e-4,\n",
    "        warmup_steps=10,\n",
    "        logging_steps=5,\n",
    "        eval_steps=50,\n",
    "        save_total_limit=1,\n",
    "        use_torch_compile=False,\n",
    "    ),\n",
    "    experiment=ExperimentConfig(\n",
    "        name=\"colab_test\",\n",
    "        use_vision=True,\n",
    "        use_bidirectional_mask=True,\n",
    "        description=\"Quick test on A100\"\n",
    "    ),\n",
    "    wandb_project=\"vision-lora-test\",\n",
    ")\n",
    "\n",
    "print(f\"Max steps: {test_config.training.max_steps}\")\n",
    "print(f\"Effective batch: {test_config.training.effective_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.model.vision_lora_model import VisionLoRAModel\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = VisionLoRAModel(test_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "import torch\n",
    "print(f\"\\nModel device: {next(model.parameters()).device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.data.dataset import VisionLanguageDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "train_dataset = VisionLanguageDataset(test_config, split=\"train\")\n",
    "val_dataset = VisionLanguageDataset(test_config, split=\"val\")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample keys: {sample.keys()}\")\n",
    "print(f\"Pixel values shape: {sample['pixel_values'].shape}\")\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "test_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "batch = {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "print(\"Testing forward pass...\")\n",
    "\n",
    "import torch\n",
    "with torch.cuda.amp.autocast(enabled=True):\n",
    "    outputs = model(**batch)\n",
    "\n",
    "print(f\"Loss: {outputs.loss.item():.4f}\")\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "print(f\"\\nMemory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Training Loop (100 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "from src.training.trainer import create_trainer\n",
    "\n",
    "print(\"Creating trainer...\")\n",
    "trainer = create_trainer(test_config)\n",
    "\n",
    "print(\"\\nStarting test training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.training.evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator(model, test_config)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"Generating captions...\")\n",
    "captions = evaluator.generate_captions(\n",
    "    val_loader,\n",
    "    num_samples=3,\n",
    "    max_new_tokens=30,\n",
    ")\n",
    "\n",
    "for i, (generated, reference) in enumerate(captions, 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(f\"Reference: {reference}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "times = []\n",
    "for _ in range(10):\n",
    "    batch = next(iter(test_loader))\n",
    "    batch = {k: v.cuda() for k, v in batch.items()}\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "samples_per_sec = 2 / avg_time\n",
    "\n",
    "print(f\"\\nPerformance Metrics (A100):\")\n",
    "print(f\"Avg inference time: {avg_time*1000:.2f} ms\")\n",
    "print(f\"Samples/sec: {samples_per_sec:.2f}\")\n",
    "print(f\"Peak memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Estimate Full Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "batch_size = 32\n",
    "gradient_accum = 4\n",
    "effective_batch = batch_size * gradient_accum\n",
    "\n",
    "target_samples_h100 = 400_000\n",
    "steps_needed = target_samples_h100 // effective_batch\n",
    "\n",
    "time_per_step_a100 = avg_time * gradient_accum\n",
    "total_time_a100_hours = (steps_needed * time_per_step_a100) / 3600\n",
    "\n",
    "h100_speedup = 1.5\n",
    "total_time_h100_hours = total_time_a100_hours / h100_speedup\n",
    "\n",
    "print(f\"\\nFull Training Estimates:\")\n",
    "print(f\"Target samples: {target_samples_h100:,}\")\n",
    "print(f\"Steps needed: {steps_needed:,}\")\n",
    "print(f\"Estimated time on A100: {total_time_a100_hours:.1f} hours\")\n",
    "print(f\"Estimated time on H100: {total_time_h100_hours:.1f} hours\")\n",
    "print(f\"\\nRecommendation: {'H100' if total_time_h100_hours < 12 else 'Reduce samples'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"gpu\": torch.cuda.get_device_name(0),\n",
    "    \"model\": test_config.model.model_name,\n",
    "    \"samples_tested\": len(train_dataset),\n",
    "    \"avg_inference_time_ms\": avg_time * 1000,\n",
    "    \"samples_per_sec\": samples_per_sec,\n",
    "    \"peak_memory_gb\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "    \"estimated_h100_time_hours\": total_time_h100_hours,\n",
    "    \"test_loss\": outputs.loss.item(),\n",
    "}\n",
    "\n",
    "output_file = \"/content/drive/MyDrive/test_results.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_file}\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}